\section{General Vocab}
\smallskip \hrule height 2pt \smallskip
 
 \begin{itemize}
 	\item \textbf{classification} - ??? Finding a f that converts X to Y where Y are categorical.  (Not regression).  
	\item \textbf{supervised learning} - at training time we are given a set of features with discrete class labels.  % Wk 4 audio transcription.
 	\item \textbf{held-out data}: 
			the terms "held-out" and "validation" are usually synonymous  % 2/25/2015 class forum (my question)
	\item \textbf{hypothesis space}: ?  E.g. binomial distribution for coin flip.  	
 	\item \textbf{prediction error}: measure of fit (?) 
	\item \textbf{regularization}: a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting  % https://en.wikipedia.org/wiki/Regularization_(mathematics)
	\item \textbf {norm}.  A scalar (not vector!) measure of distance between two vectors.  % E confirmed 3/6/2016
					You can divide by this scalar to \underline{norm}alize, but that is just one use case.
					You can also use either norm as a regularization term. 
	\item \textbf{L1} norm.  Just add the absolute values of the components.  % E confirmed 3/6/2016
	\item \textbf{L2} norm.  Can be used to regularize, or to normalizing features. 
		\begin{itemize}
			\item Euclidean vector length.  Pythagoras style. 
			\item You can do $l_2$ normalization for a feature vector to get a unit vector: 
				Convert $x$ to $\hat{x}$ so that if you form $||\hat{x}||_2^2 = 1$
				Can also do $l_1$
		\end{itemize}
	\item \textbf{L2} distance:  Pythagoras distance between two vectors. 
	\item \textbf{convergence} - if you add more data and you don't get different parameters, the model has converged. 
	\item \textbf{kernel}: some transformation of your features that improves your classification %Erick 1/30/2015
	%\item \textbf{}
	\item \textbf{affine}: indicates that the subspace need not pass through the origin.  % Intro to statistical learning Ch 9.1
	\item \textbf{support vector}: data points that “support” the maximal margin hyperplane in the sense that if these points were 				moved slightly then the maximal margin hyper- plane would move as well.   % Intro to statistical learning Ch 9.1
	\item \textbf{marginal likelihood}: 
	\item \textbf{marginalized out} = integrated out % https://en.wikipedia.org/wiki/Marginal_likelihood

 \end{itemize}
 
 Concepts:
\textbf{likelihood vs posterior}: \hfill \\

	likelihood*prior = constant*posterior.  $P(Y|X)$ is likelihood, $P(X|Y)$ is posterior.  
\hfill \\
\hfill \\

\subsubsection{Normalizing data}
Options:
\begin{itemize}
	\item min/max
	\item sigmoid
	\item $L_1$ norm
	\item $L_2$ norm
\end{itemize}
Note $L_1$/$L_2$ normalization versus penalty/regularization!!  

\subsubsection{Size for modeling P$(Y=y | X)$}
		How many parameters are needed to model $P(y | x_1, x_2, ..., x_d)$?  
		Assume Y is discrete and all the $x_i$ are binary.
		You have d binary features, so you have $2^d$ probabilities in order to classify every possible input.   % Erick
		
		The number of parameters in the PDF of $P(Y=y | X)$ is $2^d$ (+ 1 for bias sometimes).  
		The size (\# of nodes??) of the conditional probability tree grows exponentially.
		% The tree is the tree of conditional probabilities, i'll bet  % Erick. 
		The data is just a table with Y and X columns.  
		
		We only need $2^d$ (or perhaps $2^{(d+1)}$) to get all possibilities specified.  
            	If each feature can take m values instead then we have $m^d$.  Not binary any more!
		The number of parameters can get very big but Naive Bayes can handle it.  \hfill \\
		
		What is the order of the size of the parameters you need to do the full conditional probability?
		For Naive Bayes it is d; \textbf{linear}.  And the full conditional would be exponential! 

\subsubsection{Maximum Likelihood Estimation (MLE)}

\textbf{Take log, take derivative, set equal to zero.} \hfill \\

Memorize. Likelihood is DATA. \hfill \\  % e-mail to self 2/1/2015 

The best mu for a gaussian is the mean. 
Maximized probability of this data being produced by the distribution. 
The data is most likely to be generated if the mean is mu. 
Did same for sigma.  We realized best way is to use the variance.  \hfill \\

\textbf{Wikipedia:}

To use the method of maximum likelihood, one first specifies the joint density function for all observations. For an independent and identically distributed sample, this joint density function is:

$$  f(x_1,x_2,\ldots,x_n\mid\theta) = f(x_1\mid \theta)\times f(x_2|\theta) \times \cdots \times  f(x_n\mid \theta). $$
  
Now we look at this function from a different perspective by considering the observed values $x1, x2, \dots, xn$ to be fixed "parameters" of this function, whereas ? will be the function's variable and allowed to vary freely; this function will be called the likelihood:


 $$  \mathcal{L}(\theta\,;\,x_1,\ldots,x_n) = f(x_1,x_2,\ldots,x_n\mid\theta) = \prod_{i=1}^n f(x_i\mid\theta) $$
  
Note that " ; " denotes a separation between the two input arguments: $\theta$ and the observations $x_1,\ldots, x_n$.

In practice it is often more convenient to work with the logarithm of the likelihood function, called the log-likelihood:


   $$ \ln\mathcal{L}(\theta\,;\,x_1,\ldots,x_n) = \sum_{i=1}^n \ln f(x_i\mid\theta) $$
  
or the average log-likelihood:

  $$ \hat\ell = \frac1n \ln\mathcal{L} $$
  
The hat over $\ell$ indicates that it is akin to some estimator. Indeed, $\hat{\ell}$ estimates the expected log-likelihood of a single observation in the model.

The method of maximum likelihood estimates $\theta_0$ by finding a value of $\theta$ that maximizes $\hat\ell(\theta;x)$. This method of estimation defines a maximum-likelihood estimator (MLE) of $\theta_0$:


    $$  \{ \hat\theta_\mathrm{mle}\} \subseteq \{ \underset{\theta\in\Theta}{\operatorname{arg\,max}}\ \hat\ell(\theta\,;\,x_1,\ldots,x_n) \} $$

	
\subsubsection{MLE vs MAP}
\begin{itemize}
		\item both MLE and MAP are point estimates.  No estimate of uncertainty.  % https://www.youtube.com/watch?
		\item MLE fits a probabilistic model $P(x | \theta)$ to data to estimate $\theta$. % http://www.cs.colostate.edu/~cs545/fall13/dokuwiki/lib/exe/fetch.php?media=wiki%3A13_naive_bayes.pdf
			You chose the parameters $\theta$ that maximize $\ln P(X | \theta)$
		\item MLE is more likely to overfit; MAP regularizes to prevent overfitting.  \hfill \\  % https://www.youtube.com/watch?v=kkhdIriddSI 
		\item MAP doesn't have all the nice asymptotic relationship, but tends to look like MLE asymptotically.
			As your data goes to infinity, your prior foes to the data.  % https://www.youtube.com/watch?v=kkhdIriddSI 
		\item unlike MLE, MAP is not invariant under reparameterization.  (A disadvantage)  % https://www.youtube.com/watch?v=kkhdIriddSI
		\item in MAP, you have to chose a prior.  Sometimes not fun to pick.  % https://www.youtube.com/watch?v=kkhdIriddSI
\end{itemize}

\underline{TA e-mail} \hfill \\
In both of these problems, we assume that we have some i.i.d. data $\{x_1, ... , x_M\}$ which was drawn from a distribution $P(x_i | \theta)$, and we want to estimate the parameters $\theta$).

In the MLE scenario, we directly optimize the (log) likelihood of the data $P(X | \theta)$, to get the estimate of $\theta$ under which the data has maximum likelihood.

In the MAP scenario, we also have some prior knowledge about what $\theta$ should be, e.g. $P(\theta) =$ Normal$(0, \sigma)$. We can then use Bayes' rule to get $P(\theta | X)$:

$$ P(\theta | X) = P(X | \theta) P(\theta) / Z$$
where Z is the normalizing constant

That is, we want our estimate of theta to: \hfill \\
1) Model the data well,  \hfill \\
2) Have a high prior probability.  \hfill \\

\textbf{Note that when $P(\theta)$ is constant, i.e. we don't have any prior knowledge, MAP just reduces to MLE.}

\subsubsection{How Naive Bayes, MLE, and MAP fit together}  % Erick approved.
In order to do Bayesian inference, you put your likelihood into Bayes rule.  That has nothing to do with Naive Bayes.
If you just maximize the ML equation, that gives you MLE.
If you maximize the posterior that you get from Bayes rule then you have MAP.  
All that is true across analysis.

A full Bayesian analysis for many classification problems is too hard.  Too parameter rich, too computationally expensive.
You can simplify by making the Naive Bayes assumption. 
In machine learning world you will usually do this simplification. 

Note: Erick doesn't think he will ever use Naive Bayes.  
He does statistical analysis, not Naive Bayes.  
Naive Bayes is a much smaller and specialized thing than general Bayesian analysis.
That's the statistician perspective. 

For ML, it is a nice way of gettitng regularized estimates. 
You could also use Bayes to relate parameters in a model. 
That's what Erick does a lot. 

\subsubsection{Smoothing}
Can reduce sensitivity to zero values when multiplying probabilities.
\begin{itemize}
	\item prior distribution for binaries: Beta distribution. 
	\item Laplace smoothing for multinomial. 
	
\textbf{Bayes b/c you aren't going to see a feature vector that matches one in training.}
	We are \textbf{not} going to see a feature that is the exact same as a feature in the training set. 
	That's \textbf{why} we estimate w/ Bayes.   % week 5 notes
\end{itemize}

\subsubsection{Generate vs. Discriminative}
\textbf{Discriminative tries to learn $P(y | x)$, whereas a generative model tries to learn $P(x,y) = P(y|x)P(x).$ }   \hfill \\

Since the generative model learns $P(x)$ as well as $P(y|x)$, it often requires much more data, but can do things like actually generating sampled data from scratch by drawing from $P(x)$ and then from $P(y|x)$ (hence the term 'generative' model). Given that, you could classify the algorithms above as follows:

Generative: 
\begin{itemize}
	\item Naive Bayes (since we learn class distributions $P(y_i)$ and models $P(x_ij | y_i)$, a bit backwards from above, but we're still learning a generative model)
\end{itemize}
Discriminative: 
\begin{itemize}
	\item Decision Trees
	\item Perceptron
	\item SVM
\end{itemize}

Point estimation and linear regression don't really fit as well into this framework, since we're usually talking about classifiers when making this distinction. For further info you can read the discussion here: \href{http://stats.stackexchange.com/questions/12421/generative-vs-discriminative}{Stats.stackexchange}
  \hfill \\

logistic: discriminative  \hfill \\
Naive Bayes:  generative   \hfill \\
\hfill \\

One can only distinct between whether it is something, the other can say how likely. \hfill \\

Two big categories of approaches for ML: \hfill \\
\underline{Generative}:
		Those that try to estimate the joint distributions between labels and features/data. 
		Model the joint distributions. $P(X,Y) = f(X|Y) P(Y) or P(Y|X)f(x)$.  
		"Class conditional densities" are modeled.  % https://www.youtube.com/watch?v=oTtow2Ui8vg 
		\textbf{If you can create a distribution, you can sample from it.}  % wk 5 transcript
		More powerful if you have enough data to estimate the densities, but worse without enough data.
		Natural interpretation.  
		Bayes classification is an example.    
		Naive Bayes is one that can produce p(Data,Zebra), except you've made a lot of assumptions.   \hfill \\  % Wed Wk 5 transcript
		P(Data,Zebra) is a pdf over samples. 
		If you didn't relax all the constraints when going from Bayes to Naive Bayes, you could paint a zebra.   \hfill \\
		A joint probability model with evidence variable.   \hfill \\  % Lec 7: preceptrons
 \hfill \\
\underline{Discriminative}:
		No generative model, no Bayes rule, often no probabilities at all!   \hfill \\  % Preceprtons (Lec 7)
		Those that directly estimate the decision boundary.  "discriminative decision boundary".  Find $P(Y|X)$ \hfill \\
		Describing how to generate random instances X conditioned on the target attribute Y.  \hfill \\
		The discriminative classifier is like a lazy painter.  \hfill \\  % Wed Wk 5 transcript
		You can get decision boundary out of a generative model but it is over-kill if you only want to produce a label.  \hfill \\  % Wed Wk 5 transcript
		 85\% of time discriminative outperforms.  B/c p(Data,Zebra) is hard to estimate.  \hfill \\  % Wed Wk 5 transcript
		 		 
\includegraphics[width=2.5in]{figures/zebra_painting.pdf}

Why does the generative p graph max out at 0.1 and the discriminative at 1?   
    Discriminating against zebra or not zebra.  Has to sum to 1.  
    The discriminative doesn't have to sum to 1.  Many other things have to sum with it to sum to 1. 
    
\underline{TA review:} \hfill \\
In discriminative models we just want to discriminate between classes given the data: learning $P(y_i | X_i)$. 
In generative models we are trying to learn the joint distribution $P(y_i, X_i) = P(y_i) P(X_i | y_i)$, which we can then use to determine $P(y_i | X_i)$ by Bayes rule. 
Although this requires more data since we're learning $P(X_i | y_i)$, these models can be much more interpretable than discriminative models.
 
 \subsection{Linear Classifiers}
 Inputs are feature values.  \hfill \\
 Each feature has a weight.  \hfill \\
 Sum is the activation.  activation$_w(x) = \sum_i w_i x_i = w \cdot x$  \hfill \\
 If the activation is positive, chose output class 1.  \hfill \\
 If the activation is netative, chose output class 2.  \hfill \\
 
 \includegraphics[width=1.5in]{figures/linear_classifier_cartoon.pdf}  \hfill \\
 
 For a binary decision rule:   \hfill \\
 In the space of feature vectors: 
 \begin{itemize}
 	\item examples are points
	\item any weight vector is a hyperplane
	\item one side corresponds to y = +1
	\item the other side corresponds to y = -1
	\item ??? The $w \cdot x = 0$ is the solution to the line.
 \end{itemize}
 
 \includegraphics[width=1.5in]{figures/binary_decision_rule.pdf}
 
 \subsection{NB, LR, Perceptron}
\includegraphics[width=2.5in]{figures/three_views_of_classification.pdf}

\subsection{Gradient Ascent/Descent vs Coordinate Ascent/Descent}
 We discussed gradient descent with logistic regression. \hfill \\
 We mentioned coordinate descent when getting ready to talk about EM.   \hfill \\

\subsubsection{Gradient descent}
For finding the local minimum. 
\begin{itemize}
	\item Takes steps proportional to the negative of the gradient 
		(or of the approximate gradient) of the function at the current point. 
	\item If instead one takes steps proportional to the positive of the gradient, 
		one approaches a local maximum of that function; 
		the procedure is then known as gradient ascent.
\end{itemize}	

\subsubsection{Coordinate descent}
\begin{itemize}
	\item A non-derivative optimization algorithm
	\item To find a local minimum of a function, one does line search along 
		one coordinate direction at the current point in each iteration. 
		One uses different coordinate directions cyclically throughout the procedure.
	\item Has problems with non-smooth functions 
		% https://en.wikipedia.org/wiki/Coordinate_descent
	\item Coordinate descent \textbf{does} have step size parameter. % week 10 audio
		To prevent over-shooting, you many need to take smaller steps. 
	\item  Does converge under two big assumptions: \hfill \\
		(1) fixing one works.   \hfill \\
		(2) need loss to get smaller than it was before. \hfill \\
	\item Won't always converge to global optima.
	\item For coordinate descent, you have to be extremely careful that each step reduces the loss function.
		If you can't prove that you should not use coordinate descent. 
\end{itemize}	

K-means does this: alternate between holding the assignments and the centers fixed. 

