\section{Large-Margin Classifiers \& Support Vector Machines}
\smallskip \hrule height 2pt \smallskip

\begin{itemize}
	\item perceptron was online, this is batch.  % week 7 audio
	\item desire for large margins led us away from perceptrons toward this.  % week 7 audio. 
	\item There are so many lines that can separate the points. % week 7 audio
		We want good margins so it does better \textbf{on the test set}. \hfill \\  % week 7 audio
	\item We want to build the classifier that gives the biggest margins. \hfill \\  % week 7 audio
	\item Find weights $w$.    % week 7 audio
		It is a linear classifier, so form $w \cdot x$, and get sign or see what side of the line it appears.   % wk 7 audio
	\item The dot product is projection onto the line.   ??? (use better vocab).  
		\begin{itemize}
			\item $y$ is the direction to go 
			\item $w \cdot x$ is projection
			\item $w_0$ is bias
		\end{itemize}
	\item label times prediction
	\item Line vs classifier: can run perceptron to get a line,   % wk 7 audio
			then move it around to get the high margin one. 
		But you don't need to do this (b/c convex optimization.)
		(HW 3 Q1 may be the by-hand version.)
	\item % wk 7 audio
		Need to normalize the weights $w$. 
		There are an infinite ways to write the same line.
		As you increase the magnitude of the weights, the dot product goes up. 
		You can always increase w and get a better objective function.
		\textbf{so you need to prevent the optimizer from getting an easy way out.}
		Maximizing gamma (minimizing $||w||_2$) takes care of this. 
	\item maximizing the margin is minimizing the norm2 of the weight vector subject to the constraint that we don't like to make mistakes.  % wk 7 audio
	\item There are very few points that play a role in the formation of the hyperplane.
		Those points are called support vectors.
		The points far from the line play no role.  % wk 7 audio.\
	\item with SVM, if you assume linearly seperable, you come up w/ line that maximizes the margin.   % wk 7 audio 
	\item Can think of SVMS of minimizing loss + regularization.   % wk 7 audio
		No closed form solution but convex, concave, can solve w/ quadratic programming.  
		Standard solvers. 
	\item Two ways of dealing with nonlinearity: soft margin and kernels
	\item Desire for nonlinear boundary $\rightarrow$ kernel.
\end{itemize}

\subsubsection{what happens with very small w?}
 Small $w \rightarrow$ big margin $\rightarrow$ poor performance.


Vocab: 
\begin{itemize}
	\item \textbf{Large margin classifier}:
	\item \textbf{Support Vector}:
	\item \textbf{Support Vector Machine}:
	\item \textbf{hard SVM}: don't want \underline{anything} inside your margin. 
	\item \textbf{soft SVM}: softer: compute distance to hyperplane. 

\end{itemize}

Notation:
\begin{itemize}
	\item $i$ or $j$: the $i^{th}$ or $j^{th}$ data (training) point. 
	\item $\bm{w}$: the weights of the model.  The black line is perpendicular to this vector. 
	\item $\bm{w} \cdot \bm{x}$: the distance from x to the decision boundary. 
	\item $\bm{w} \cdot \bm{x} + w_0$: to move the decision boundary off the origin, you need to shift it by a constant.  
	\item $\bm{w}_0$: a constant that defines a line parallel to the decision boundary and is $w_0$ units away. 
\end{itemize}

\subsection{Intro}
Like perceptron, but maximizes the margin.   \hfill \\
\hfill \\

Optimizing a small weight vector: $\displaystyle min_w \: \frac{1}{2}||w||^2$ \hfill  \\  % \: gives 4/18ths of \quad space.
	% https://www.andy-roberts.net/res/writing/latex/hspacing.pdf
and getting points right: $\forall \mbox { points } i, y$  $w_{y^*} \cdot x^i \geq w_y \cdot x^i + 1$ \hfill \\
\hfill \\
Summary: \hfill \\
\includegraphics[width=2.5in]{figures/svm_overview.pdf}

There are many possible ways to write the same line: \hfill \\
If $\bm{w} \cdot \bm{x} + w_0 = 0$, then these also work: 
$2\bm{w} \cdot \bm{x} + 2w_0 = 0$, $1000\bm{w} \cdot \bm{x} + 1000w_0 = 0$, $\dots$ \hfill \\
Any constant scaling has the same intersection with the z = 0 plane, so you get the same dividing line. \hfill \\
\textbf{This is why we \underline{don't} want} max$_{\gamma, w, w_0}$. \hfill \\  \hfill \\

Want a $w$ so that when you multiply by $x^j$, the sign is similar to $y^j$ but also gamma away.
Nonzero gamma $\rightarrow$ margin.  
If you used $0$ instead of $\gamma$, you would basically just be doing the perceptron.

\hfill \\   \hfill \\
%Recall that the distance from the the decision boundary to a point is given by $\gamma$
%\includegraphics[width=2.5in]{figures/svm_dist_to_plane.pdf}

The distance from $x^j$ to the decision boundary is given by $\lambda \frac{w}{||w||_2}$ :  \hfill \\
\includegraphics[width=1.5in]{figures/svm_component_norm_to_decision_boundary.pdf}  \hfill \\
$\bar{x}^j$ is the component of $x^j$ that is normal to $w$. \hfill \\
So $x^j = \bar{x}^j + \lambda \frac{w}{||w||_2}$  \hfill \\
(Recall $||w||_2 = \sqrt{\sum_i w_i^2}$)  \hfill \\   \hfill \\

\subsection{motivation for minimizing the norm of the weights}
We can maximize the margin by minimizing $||w||_2$: 
$\gamma = \frac{||w||_2}{w \cdot w} = \frac{1}{||w||_2}$   \hfill \\
Derivation:  \hfill \\
\includegraphics[width=2.7in]{figures/svm_derivation_of_minimizing_weights.pdf} \hfill \\

Intuitive explanation, after 2 key facts:  \hfill \\
\begin{itemize}
	\item Key fact \#1: the bigger your $\bm{w}$ is, the \underline{closer} the line $\bm{w}\bm{x}=1$ is to the line $\bm{w}\bm{x} = 0$.  		Small distance between $\bm{w}\bm{x}=1$ and $\bm{w}\bm{x}=0$ translates to a small margin.  
		Note that you could use any constant in place of 1. 
	\item Key fact \#2: The $w_0$ just shifts the black decision boundary line away from the origin.  
\end{itemize}
Now we can explain why minimizing the norm of the weights leads to the largest margin. 

\begin{itemize}
	\item Imagine $w_0 = 0$, meaning the decision boundary goes through the origin.  \hfill \\
	\item Now let $w = [2, 0]$ for simplicity.    
		Where is the decision boundary?  
		The decision boundary corresponds to $2 x_0 + 0 x_1 = 0$.  
		That means the decision boundary is along $x_0 = 0$, which is a line through the $x_1$ (vertical) axis. 
	\item But the point is that when you set $\bm{w} \cdot \bm{x} = c$ or some other constant.
		Key fact \#1 says that the larger w is, the farther $\bm{w} \cdot \bm{x} = 0$ is from $\bm{w} \cdot \bm{x} = c$  
		In this case where $w_1 = 0$, you have  $w_0 x_0 = c$, or $x_0 = c/w_0 = c/2$.  
		If you want the $=1$ line far from the decision boundary, you need to increase the distance between these lines.
		Since $c$ is fixed, so the only way we can do this is to decrease $w_0$. 
	\item The logic holds true for when $w_1$ is nonzero: you just want to minimize the size of the $\bm{w}$ vector.
		Minimizing the size of $w$ is equivalent to minimizing $||w||_2$ 
\end{itemize}

\includegraphics[width=2.7in]{figures/max_margin_using_canonical_hyperplanes.pdf}
 
The bottom line under the arrow keeps the $w$ values from getting too small.   % wk 7 audio
\hfill \\
The 1/2 is just because we will take the derivative.  % wk 7 audio

Previously we were just trying to maximizing margin.  
That was equal to minimizing the norm of $w$. 
Then we had a $\geq \gamma$ but now we have $\geq 1$. 
 Are we still maximizing the margin?  
 Yes; we showed minimizing the norm of $w$ leads to the max margin.
 \textbf{Part is pushing for bigger margin, part is keeping points from being inside the margin.}
 
The only points that have nonzero weights are the ones that are very close to the margin. \hfill \\
The line only changes if you remove points close to the line. \hfill \\
\hfill \\

There are few points in the training data that are crucial for the line placement. 
Those points are called support vectors
The rest of the points would not play a role. 
We call them support vectors b/c those are the points that can support the decision boundary. 

\subsection{SVM recipe}
We want to minimize the norm subject to getting the predictions right:  \hfill \\
$\displaystyle  \min_{w, w_0} \frac{1}{2} ||w||_2^2$ so that $\forall j . y^j(w \cdot x^j + w_0) \geq 1$

We do this with quadratic programming (QP). 
The decision boundary is defined by \textbf{support vectors}, which are data points on the canonical red lines. 
All the points that aren't on the red line are non-support vectors. \hfill \\
 \hfill \\
 If your data is not linearly separable, you can add nonlinear features.
 These are called Kernels, and will be discussed later. 
 \hfill \\ \hfill \\
 
 We were minimizing the $norm_2$ of $w$ subject to constraint that $y_i (w \cdot x) > 1$
\hfill\\ \hfill \\
 
 \subsection{Balancing \# of mistakes and $||w||_2^2$}
We want to soften the constraint to allow for occasional mistakes.
This doesn't happen by changing the 1 to 1/2.  
Reduce the number of points that have to be good. 
Introduce a new variable that says how wrong you are.  \hfill \\
\hfill \\
 
 If your data isn't linearly separable, then you might need to allow your $||w||_2$ to be a little bigger and make a few mistakes. 
One option (not what we end up doing): 
 $\displaystyle  \min_{w, w_0} \frac{1}{2} ||w||_2^2 + M$ (M = \# of mistakes)  \hfill \\
 so that $\forall j . y^j(w \cdot x^j + w_0) \geq 1$ \hfill \\
  
 \subsection{Hinge Loss}
\textbf{Zero loss for things you get correct, then penalty grows linearly if you make a mistake.}
\hfill \\  \hfill \\

Want $1 - \xi = 0$ for when we are right. 
 Use the margin: $\xi$ is the perpendicular distance from point to hyperplane.
 Now our problem looks like: \hfill \\
 $\min ||w||_2^2$ subject to constraint $y_i(w \cdot x_i) > 1 - \xi$
 But we need to adjust it so we don't get a trivial solution.
 We need to adjust $\xi$. 
 \hfill \\  \hfill \\ 
 
 One way to balance the number of mistakes and how big $w$ is:  
  $\displaystyle  \min_{w, w_0} \frac{1}{2} ||w||_2^2 + C \sum_j \xi^j$ \hfill \\
 so that $\forall j . y^j(w \cdot x^j + w_0) \geq 1 - \xi^j$ \hfill \\
 C = strength of penalty. \hfill \\
 $\xi$ is size of error for each error.  If the point is classified correctly, $\xi$ is zero.   \hfill \\
 \hfill \\
 This is \textbf{"soft margin"} SVM.
\hfill \\   \hfill \\
 
 We have a sum because we want to have small $\xi$  
Want both of them to be small. 
We want most of $\xi$ values to be zero. 
At same time we also want norm of $w$ to be small.  \hfill \\
\hfill \\
 
 Note: we don't want to sum over $\xi$ squared.
 We don't want extra big penalty if one is way off.
 We can use scaling to help. 
 \hfill \\ \hfill \\
 
That point on the hinge is sharp.  Can replace it with \_\_\_\_ to make it softer. 
If you make no mistakes, then you get all zeros.
That means your data is linearly separable.  \hfill \\   \hfill \\

 
 When we want the dot product \& shift $\geq 1 - \xi^j$, the 1 is for the green-line margin, and you are wrong by $\xi$  amount. \hfill \\
 \includegraphics[width=1.5in]{figures/hinge_loss_xis.pdf}
 \hfill \\
 Now we can tune our balance between asking for small weights $\bm{w}$ and asking for perfect classification. 
 \begin{itemize} 
 	\item $C  = \infty \rightarrow$ pressure to separate the data, even if the margin is skinny.
	\item $C = 0 \rightarrow$ fat margins over accuracy. 
\end{itemize}
Use your training set to train C.  \hfill \\ 
\hfill \\

If there exists a + on the wrong side, penalize the distance to the margin or hyperplane? 
Typically use distance to the hyperplane.  Just a difference of 1. 
\hfill \\  \hfill \\

Note that if \_\_\_\_\_ $\geq 1$, you don't care if the point is classified wrong.   % had used "margin" but this doesn't make sense!
But if \_\_\_\_\_ $ < 1$, you pay a linear penalty. % had used "margin" but this doesn't make sense! 
\hfill \\

\textbf{Hinge Loss:}  \hfill \\
$\displaystyle \min_{w, w_0} \frac{1}{2} ||w||_2^2 + C \sum_{j=1}^N [1 - y^j(w \cdot x^j + w_0)]$ \hfill \\
$1^{st}$ term is regularization, $2^{nd}$ term is hinge loss.  \hfill \\
\hfill \\

Steps:
\begin{enumerate}
	\item extract features
	\item sweep parameters w/ k-fold validation. 
	\item solve SVM
\end{enumerate}
\hfill \\

Solve by differentiating and set equal to zero.  \hfill \\
There is no closed form solution, but quadratic program is concave.  (??)   \hfill \\
Hinge loss is not differentiable (??), so gradient ascent is a little trickier.  \hfill \\
\hfill \\

This won't handle super messy tangled clouds, but hard SVM said don't want \underline{anything} inside your margin. 
Softer: compute distance to hyperplane.  \hfill \\
\hfill \\

\subsubsection{Logistic Regression to Minimize Loss}
Logistic regression assumes $P(Y=1 | X=x) = \frac{exp(f(x))}{1 + exp(f(x))}$  \hfill \\
(For Logistic Regression, $f(x)$ was $w_0 + \sum_i w_i X_i$ and we had $Y = \{0, 1\}$)  \hfill \\
Now we have $Y = \{ -1, +1 \}$.  \hfill \\
To maximize data likelihood for $Y = \{ -1, +1 \}$: \hfill \\
$\displaystyle  P(y^i | x^i) = \frac{1}{1 + exp(-y^i f(x^i))}$ \hfill \\
\begin{align*}
	\ln P(D_Y | D_{\bm{X}}, \bm{w}) &= \sum_{j=1}^N \ln P(y^j | x^j, w) \\ 
		& \mbox{plug in the $P$ above} \\
		&= - \sum_{i=1}^N \ln(1 + exp(-y^i f(x^i)))
\end{align*}
Since $-\ln(z) = \ln(1/z)$ we get to minimize this (negative):   \hfill \\
$\displaystyle  \sum_{i=1}^N \ln(1 + exp(-y^i f(x^i))) =  \sum_{i=1}^N \ln(1 + exp(-y^i [w_0 + \sum w_i x_i]))$

\subsubsection{SVMs vs Regularized Logistic Regression}

Logistic regression is a way to minimize loss.
The probability of data point being class 1 is $\dots$ sigmoid function. 
Typically that function is a linear combination. 
We maximized the likelihood of the training data: maximize $w$.
At the end you are minimizing the $\ln()$ below. \hfill \\
\hfill \\

SVM objective: minimizing over $w$, for margin + hinge los.
LR: loss is not hinge any more.  Loss is exponential. 
But we are actually doing very similar shapes.  \hfill \\
\hfill \\


\textbf{SVM Objective:} \hfill \\
$\displaystyle \argmin_{\bm{w}, w_0} \frac{1}{2} ||w||_2^2 + C \sum_{j=1}^N [1 - y^j f(x^j)]_+$ \hfill \\
where $[x]_+ = max(x , 0)$ \hfill \\
The $[x]_+$ is made for ease of notation.
 Just a mathematical way of writing that curve.  \hfill \\
 \hfill \\

\textbf{Logistic Regression Objective:} \hfill \\
$\displaystyle  \argmin_{\bm{w}, w_0} \lambda ||w||_2^2 + \sum_{j=1}^N ln(1 + \exp(-y^j f(x^j)))$ \hfill \\
 \hfill \\
 
 Note that SVM and Logistic Regression have the same $l_2$ regularization term, but different error terms. 

 \includegraphics[width=2.5in]{figures/LR_svm_step_losses.pdf} \hfill \\
 The red curve is true decision boundary.
 Green is hinge loss
 Blue is soft version. 
 So the 2 are optimizing similar things.  
 Trying to approximate the step function.  \hfill \\
 \hfill \\
 
LR starts penalizing you a bit when you get close to the decision boundary.
But once you hit the margin, SVM penalizes more.  
Then they flip back again after the lines extend past what's drawn on the left. 
SVM has hinge, LR has exponential or logistic function.   \hfill \\
\hfill \\
 
 \subsection{Multi-class SVMs}
 To do 3 classes, you need to learn 3 classifiers. \hfill \\
 Can't just do $y = \argmax_i w_i \cdot x$ for $i$ classifiers.  
 Wouldn't handle this: 
 \includegraphics[width=0.6in]{figures/multiclass_svm_motivation.pdf}
 \hfill \\  \hfill \\
 
 Instead, we learn 3 classifiers for these 3 symbols: 
 \begin{enumerate}
 	\item + vs $\{ O, - \}$, weights $w_+$
	\item + vs $\{ O, + \}$, weights $w_-$
	\item + vs $\{ +, - \}$, weights $w_O$
 \end{enumerate}
 But to get it working for that set of 3 columns, we need additional constraints.  \hfill \\
 For each class: \hfill \\
 for class $y'$ that is not class $y^j$ ($\forall y' \neq y^j$):  \hfill \\
 And for all classes $j$ ($\forall j$):  \hfill \\
 $w^{y^j} \cdot x^j + w_0^{y^j} \geq w^{y'} \cdot x^j + w_o^{y'} + 1$. \hfill \\
 ($\forall$ = "for all") \hfill \\
 In plain english: ????.   \hfill \\
 ??? Do I have the fact that j is for classes right?  (Could j still be points?)  ??
 \hfill \\
 \hfill \\
 
We can also introduce slack variables as before. 
$\displaystyle \min_{w, w_0} \sum_y ||w^y||_2^2 + C \sum_j \xi^j$ \hfill \\
$w^{y^j} \cdot x^j + w_0^{y^j} \geq w^{y'} \cdot x^j + w_o^{y'} + 1 - \xi^j$. \hfill \\
That's true for class $y'$ that is not class $y^j$ ($\forall y' \neq y^j$), and all classes $j$ ($\forall j$) and for all $\xi^j > 0$ \hfill \\
 \hfill \\
 
 So you can do multiple classes in a one against all approach *or* a multiclass SVM approach.  
 
 \subsubsection{Audio from week 7:}
 How can we change our SVM formulation to account for multiple classes? 
    What do you do to your $y_i(w \cdot x) \geq 1 - \xi_i$?
    Want $\hat{y_i}$. 
    If we get right w then all the blues should be on the left.  
    so $w_{blue} x_{blue} > w_{any other x_i}$.  
        For this green point, want the blue one to be responsible for the classification. 
        Want the $w \cdot x$ for the right classifier to be better than any other by a margin of 1.  
\hfill \\   \hfill \\

How is different than 1 vs 3 or 1 vs all? 
    In 1 vs all, making independent predictions for categories. 
    Here we are simultaneously learning them all at the same time. 
    Training 3 $w$s at same time, together. 
    For blue sample, $w$ of blue times feature of that sample should be bigger than 
        "1 + the margin"  (??). 
    Why is it + 1? 
        Margin.  Want all other points to be at least one away. 
\hfill \\ \hfill \\

How many $\xi$ values for $n$ data point? 
    for binary SVM, have $n$. \# of samples.
    For multi-class, $n$ times number of classifiers that we are training
        Each classifier has its own $\xi$.  
        We could sum it up and consider 1 if we want.  Both are options.
\hfill \\ 

\textbf{We can do multi-class with a soft margin as well}
Don't want any of the non-blue classifiers to claim it is $w$. 
    Pushing the red, green lines away from blue points. 
    Want $w$ to not like any of the blues.  
    At least not more than the blues by a margin of 1. 
\hfill \\

 
 \subsection{SVM info from other sources}
 \subsubsection{http://axon.cs.byu.edu/Dan/478/misc/SVM.example.pdf}
 The idea behind SVMs is to make use of a (nonlinear) mapping function that transforms data in input space to data
in feature space in such a way as to render a problem linearly separable.  % http://axon.cs.byu.edu/Dan/478/misc/SVM.example.pdf
The SVM then automatically discovers the optimal separating hyperplane (which, when mapped back into input space, can be a complex decision surface). % http://axon.cs.byu.edu/Dan/478/misc/SVM.example.pdf

 \subsubsection{An Introduction to Statistical Learning}
 \begin{itemize}
 	\item shown to perform well in a variety of settings, and are often considered one of the best “out of the box” classifiers.
	\item The support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier. 
		Though the maximal margin classifier is elegant and simple, 
			it cannot be applied to most data sets because it requires that the classes be separable by a linear boundary.
	\item People often loosely refer to the maximal margin classifier, the support vector classifier, 
			and the support vector machine as “support vector machines”. 
	\item Although the maximal margin classifier is often successful, it can also lead to overfitting when p is large.
	\item \textbf{support vector}: data points that “support” the maximal margin hyperplane in the sense that if these points were 				moved slightly then the maximal margin hyper- plane would move as well.   % Intro to statistical learning Ch 9.1
	\item In many cases no separating hyperplane exists, and so there is no maximal margin classifier. 
	\item If you cant separate perfectly, you can settle for almost separating the classes using a so-called soft margin.
	\item Even if a separating hyperplane does exist, then there are instances in which a classifier based on a 
			separating hyperplane might not be desirable. 
	\item The fact that the maximal margin hyperplane is extremely sensitive to a change in a single observation 
			suggests that it may have overfit the training data.
	\item The support vector classifier, sometimes called a \textbf{soft margin classifier}: \hfill \\
		Rather than seeking the largest possible margin so that every observation is not only on the correct side 
			of the hyperplane but also on the correct side of the margin, we instead allow some observations to 
			be on the incorrect side of the margin, or even the incorrect side of the hyperplane. 
		(The margin is soft because it can be violated by some of the training observations.) 
	\item An observation that lies strictly on the correct side of the margin does not affect the support vector classifier.
		Changing the position of that observation would not change the classifier at all, 
			provided that its position remains on the correct side of the margin. 
	\item The \textbf{support vector machine} (SVM) is an extension of the support vector classifier that results from 
		enlarging the feature space in a specific way, using \textbf{kernels}.
	\item A \textbf{kernel} is a function that quantifies the similarity of two observations.
	\item What is the advantage of using a kernel rather than simply enlarging the feature space 
			using functions of the original features? 
		One advantage is computational, and it amounts to the fact that using kernels, 
			one need only compute K(xi, xi? ) for all (n choose 2) distinct pairs i, i?. 
		This can be done without explicitly working in the enlarged feature space.
		This is important because in many applications of SVMs, 
			the enlarged feature space is so large that computations are intractable.
 \end{itemize}
 


 
  



