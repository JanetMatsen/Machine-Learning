\section{Naive Bayes}
\smallskip \hrule height 2pt \smallskip
Parameters are from data statistics; probabilistic interpretation.  \hfill \\ % lec 7 end
Train on Y conditioned on x. \hfill \\  % Friday wk 4 audio transcription.
Bayes: $P(Y=X) \propto P(X|Y)P(Y)$.  
But that is ($2^N$) parameters, and you might not be able to make a model with that many ($N$) parameters if your training data has a lot of features.
So you can add some assumptions that take you from $2^N$ parameters down to $2N$ of them. \hfill \\  % Friday wk 4 audio transcription.

Study the problem feature by feature.  Assume chunks of features are conditionally independent.  This is a false assumption, but often works.  
(Features are probably not independent, but it turns out to be reasonably safe to assume so.)
MAP is the foundation for Naive Bayes classifiers.  % http://www.cs.cmu.edu/~tom/10601_sp08/slides/recitation-mle-nb.pdf
 \hfill \\
  \hfill \\
Do we do optimization with NB?  
No.  We start with the joint distribution: $P(x|y)*prior$, $\dots$, take the max.  % week 6 audio
    Not ML optimizing.  Not required to optimize every time we learn something. % week 6 audio

Vocabulary
\begin{itemize}
	\item \textbf{$h_{NB}(x)$} the function that returns the best class.  %Erick 2/4/2016
	\item Loss function:
	in Naive Bayes the loss function is the negative log likelihood of the data given the parameters: $- \ln(P(X, y | w))$.  % TA e-mail 3/15/2016
\end{itemize}

Advantages:
\begin{itemize}
	\item Fast to train (single scan/pass through data). Fast to classify  % http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
	\item Not sensitive to irrelevant features  % http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
	\item Handles real and discrete data   % http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
	\item Handles streaming data well   % http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
	\item conditional independence assumption $\rightarrow$ we don't need to see occurrences of joint assignments to estimate their probabilities.  % HW 2 forum
\end{itemize}
Disadvantages:
\begin{itemize}
	\item Assumes independence of features  % http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
	\item All observations have equal weight in prediction. % http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
\end{itemize}

Vocab:
\begin{itemize}
	\item \textbf{prior}:  $P(Y)$, the probability of a label/class.
	\item \textbf{Likelihood}: $P(\bm{X} | Y)$. 
	%\item 
	%\item \textbf{}
\end{itemize}
 
\subsubsection{Conditional independence}
$X$ is conditionally independent of $Y$ given $Z$ if the probability distribution
for $X$ is independent of the value of $Y$, given the value of $Z$: \hfill \\
$(\forall i, j, k)$ $P(X=i | Y = j | Z=k) = P(X=i | Z=k)$.  \hfill \\
E.g. P( Thunder $|$ Rain, Lightening) = P(Thunder $|$ Lightning) \hfill \\
Equivalent to P(X, Y $|$ Z) = P(X $|$ Z) P(Y $|$ Z)  \hfill \\ \hfill \\

TODO: put in plain english. 

HW 2 forum: \hfill 
Naive Bayes assumption (is): \hfill \\
$P(D1, D2|H) = P(D1|H)P(D2|H)$, everything else follows from the rules of probability.


\subsubsection{Naive Bayes Assumption}
Features are independent given the class: \hfill \\
\begin{align*}
	P(X_1, X_2 | Y) &= P(X_1 | X_2, Y) P(X_2 | Y)  \\
				&= P(X_1 | Y) P(X_2| Y)      \\
				& \mbox{more generally:}      \\
	P(X_1, \dots, X_n | Y) &=  \prod_i P(X_i | Y)			
\end{align*}

This reduces the number of parameters a lot!
Say you had 5 features.  
Before this assumption, each of your 5 features could be dependent.   Then you have to assign a probability to each state.  If each is binary, then you can say $2^5$.  
After this assumption, you would just have 5 parameters.   \hfill\\ \hfill \\

\subsubsection{Homework clarifications}
\underline{Homework 2 TA notes:}  \hfill \\
Naive Bayes gives us a way to compute the whole joint distribution P(Cold, Headache, Cough, SoreThroat). Once we have this, everything else follows from the sum/chain rules of probability:

From the definition of conditional probability (which is the chain rule in disguise):
P(Cold | notHeadache, Cough, SoreThroat) = P(Cold, not Headache, Cough, SoreThroat)/P(not Headache, Cough, Sore Throat)

From the sum rule we see that the denominator is:
P(not Headache, Cough, SoreThroat) = P(Cold, not Headache, Cough, SoreThroat) + P(not Cold, not Headache, Cough, SoreThroat)

Thus the entire conditional probability can be computed using the joint distribution (which can be computed from the factorization and the estimates from the dataset).

The reason we want to include the denominator is to make sure that P(not Headache, Cough, SoreThroat) is non-zero (as asked in another post). It's not zero here, but if it was that would mean we can't really ask about the prediction P(Cold | not Headache, Cough, SoreThroat), since this is only defined when P(not Headache, Cough, SoreThroat) is non-zero. In practice people just assume this isn't an issue, and directly maximize the joint probability.

\hfill \\
The point of Naive Bayes isn't that $P(H| D1, D2)$ is proportional to $P(H, D1, D2)$, as this is true for any distribution (Here Di are the data, H is the hypothesis for some arbitrary problem). The point is that we assume the conditional independence $P(D1, D2| H)$ is $P(D1|H) P(D2|H)$. No the denominator is not used in practice, but that's a simplifying assumption that only works if none of your conditional probabilities are zero (which in turn means all joint probabilities are nonzero). Using the denominator doesn't defeat the point of Naive Bayes, it just ensures that $P(Cold | not Headache, Cough, SoreThroat)$ is well defined as explained above. \hfill \\

You should be using the conditional independence assumption from Naive Bayes to compute the joint probabilities in the numerator/denominator: e.g. $P(H, D1, D2) = P(H)*P(D1|H)*P(D2|H)$, where $P(H)$, $P(D1|H)$, $P(D2|H)$ are estimated from the data (with or without Laplacian smoothing). The conditional independence assumption means that we don't estimate $P(D1 = True, D2 = False | H)$ from the number of times we see $(D1= True, D2=False)$ together, we split $P(D1=True, D2=False|H)$ into $P(D1=True|H)*P(D2=False|H)$ and estimate each conditional independently. This is a huge simplification, since if we have N binary variables, we would need to see $2^N$ combinations to see all of the joint assignments (D1,...DN).
\hfill \\ \hfill \\

[Cold] is not independent of [Headache], [Cough], or [SoreThroat]. Consider: if [Cold] were in fact not related to these variables, we wouldn't be predicting [Cold] from them. Furthermore, [Cough] and [Headache] are not independent, since both are not independent of [Cold]. However, the Naive Bayes assumption is that once you account for that dependence, [Cough] and [Headache] are independent.

Here's a common-sense description of this. If you go around talking to people, you notice that some have headaches, and some have colds. At first, you are confused and wonder why this might be. Is it, perhaps, that coughing all the time is so annoying that it eventually causes people headaches? But then you realize that there is a common illness, the cold, that causes both headaches and coughs. Aha! you say. So you start asking people not only if they have a headache and a cough, but also whether or not they have a cold. You find that among people with colds, having a headache and having a cough seem like unrelated phenomena. And among people without colds, headaches and colds are again unrelated. This is the world that Naive Bayes assumes.

You ask "Would it be the case then that P(Cd, H, C, S) = P(H, C, S | Cd) P(Cd) ?" Yes. That fact is the definition of conditional probability, and it is a basic fact of probabilities. It is true for all variables, in all situations, no matter what assumptions you make. It is true whether Cd and H are independent or not, whether they represent colds or aliens or whatever. You can always use this fact about any variables. \hfill \\
-------------

Naive Bayes is not about ignoring the denominator. This seems to have been a common misunderstanding.

Naive Bayes is an independence assumption; namely, that your input features are independent given the output feature.

This assumption allows you to estimate probabilities like P(X, Y, Z) in terms of other probabilities P(X, Y) and P(X, Z). Since it relates probabilities to each other, it makes it easier to estimate probabilities from samples. For example, if you are trying to detect if a message is spam, you want to compute P(spam | Hello, I, am, Prince, Albert, of, Nigeria, ...), and you may have never seen any points with text (Hello, I, am, Prince, Albert, of, Nigeria, ...) and so would not have any estimate of the probability that that message is spam. With Naive Bayes, you would rewrite this to in terms of the probabilities P(spam | Hello) and P(spam | I) and P(spam | am) and ..., all of which you can compute because you have seen many messages with the words "Hello", or "Prince", or "Nigeria", and know the associated probabilities of spam.

This question asks you to use the Naive Bayes assumption to estimate a probability value given some data, which is exactly what you use Naive Bayes for.

\subsubsection{Naive Bayes Classifier}
Given:
\begin{itemize}
	\item a prior $P(Y)$  
	\item $n$ conditionally independent features $\bm{X}$ given the class $Y$
	\item calculated likelihood for each $X_i$ of the form $P(X_i | Y)$
\end{itemize}
Your decision rule is:   (note $h_{NB}$ is Naive Bayes, not Neg Binom) 
\begin{align*}
	y^* = h_{NB}(x) &= \argmax_y P(y) P(x_1, \dots, x_n | y)   \\
			&=  \argmax_y P(y) \prod_i P(x_i | y)
\end{align*}

\noindent
Although the assumption that the predictor (independent) variables are independent is not always accurate, it does simplify the classification task dramatically, since it allows the class conditional densities $p(x_k | C_j)$ to be calculated separately for each variable, i.e., it reduces a multidimensional task to a number of one-dimensional ones.
In effect, Naive Bayes reduces a high-dimensional density estimation task to a one-dimensional kernel density estimation. Furthermore, the assumption does not seem to greatly affect the posterior probabilities, especially in regions near decision boundaries, thus, leaving the classification task unaffected.
% http://www.statsoft.com/textbook/naive-bayes-classifier

Na•ve Bayes is NOT sensitive to irrelevant features.  % http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
However, this assumes that we have good enough estimates of the probabilities, so the more data the better.

\subsubsection{Digit classification example}
Simplify images of digits to pixels, and assign them True or False for whether they are "on".  \hfill \\
Each input maps to a feature in a vector.  E.g. pixel in the 0th for and 0th column is $F_{0,0}$.   \hfill \\
The Naive Bayes model is: \hfill \\
$ \displaystyle P(Y | F_{0,0}, \dots , F_{15,15}) \propto P(Y) \prod_{i,j} P(F_{i,j} | Y)$.
We assume the features are independent given the class.  
We need to learn the distribution of pixels on at each pixel given each number.  \hfill \\
How to calculate the prior, P(Y): \hfill \\
$\displaystyle P(Y) = \frac{count(Y=y)}{\sum_{y'} Count(Y=y'}$ (denominator is summing over all y values) \hfill \\
How to calculate the likelihood:  \hfill \\
$\displaystyle P(X_i =x | Y=y) = \frac{Count(X_i=x, Y=y)}{\sum_{x'}Count(X_i=x', Y=y)}$


\subsubsection{For binary features, use the Beta prior and MAP.} 
Just like likeihood of binomial previously! 
$\displaystyle P(\theta | D) = \frac{\theta^{\beta_H + \alpha_H - 1}(1-theta)^{\beta_T + \alpha_T - 1}}{B(\beta_H + \alpha_H, \beta_T + \alpha_T)} \approx Beta(\beta_H + \alpha_H, \beta_T + \alpha_T) $
Chose $\theta$ using MAP:  \hfill \\
$\displaystyle  \widehat{\theta} = \argmax_\theta P(\theta | D) = \frac{\alpha_H + \beta_H - 1}{\alpha_H + \beta_H + \alpha_T + \beta_T - 2}$.  \hfill \\
Once again, the Beta prior is equivalent to adding extra observations for each feature. \hfill \\
If you don't have a lot of observations, the prior is important.  \hfill \\
And as the number of observations goes to $\infty$, the prior is "forgotten". 

\subsubsection{Multinomials: Laplace Smoothing}
\underline{Laplace's estimate}: \hfill \\
Pretend you saw every outcome $k$ extra times: \hfill \\
$\displaystyle P_{LAP, k}(x) = \frac{c(x) + k}{N + k|X|}$ \hfill \\
$N = $ number of observations.  \hfill \\
$|X| = $ the number of categories you are counting.  \hfill \\
$k$ is the strength of the prior; how much of the prior information you are going to enforce\hfill \\  \hfill \\

Example: 
$\displaystyle P_{LAP, 0}(X) = \langle \frac{2}{3}, \frac{1}{3} \rangle$.  \hfill \\
Set $k=1$.  $|X|$ is 2.  $N$ is 3.  
$\displaystyle P_{LAP, 1}(X) = \langle \frac{3}{5}, \frac{2}{5} \rangle$  \hfill \\
$\displaystyle P_{LAP, 100}(X) = \langle \frac{102}{203}, \frac{101}{203} \rangle$  \hfill \\

\underline{Laplace for conditionals:}
Smooth each condition independently:  \hfill \\
$\displaystyle  P_{LAP, k}(x|y) = \frac{c(x,y) + k}{c(y) + k |X|}$

\subsubsection{Subtleties of the NB classifier}
\textbf{(1) Usually the features are not conditionally independent}: \hfill \\
$P(X_1, \dots, X_n | Y) \neq \prod_i P(X_i | Y)$ \hfill \\
The actual probabilities $P(Y | \bm{X})$ are often biased towards 0 or 1.  
Nonetheless, NB is the single most used classifier out there.  
It performs well even when the independence assumption is violated.  \hfill \\
\textbf{(2) Overfitting} \hfill \\
Conditional probabilities can easily be calculated as zero. 
Zero probabilities kill that class' chance at being called. \hfill \\
??? If the feature is binary, we can use MAP with a beta prior. 
??? That's equivalent to adding extra observations for each feature. 

\subsubsection{NB for text classification}
\begin{itemize}
	\item Need a feature vector with a suitably small number of features.
		Bag of words model is commonly used. 
	\item $i$ is the $i^{th}$ word
	\item NB assumption(\_\_\_) helps a lot.  
		$P(X_i=x_i |Y=y)$ is just the probability of observing word $x_i$ in a document on topic $y$.  
		$ \displaystyle  h_{NB}(x) = \argmax_y P(y) \prod_{i=1}^{LengthDoc} P(x_i | y)$  
	\item Additional assumption: bag of words model. \hfill \\
		Order of words ignored.  Works really well.  \hfill \\
		$P(X_i = x_i | Y=y) = P(X_k = x_i | Y=y)$  ($k$ is the $k^{th}$ word (?); all positions have the same distribution).  \hfill \\
		$P(y) = \prod_{i=1}^{LengthDoc} P(x_i | y)$
	\item Prior, $P(Y)$, is the fraction of documents of each topic.
	\item Likelihood, $P(X_i | Y)$ is count for how many times you saw the word in documents of this topic.  
		This distribution is shared across all positions $i$.
	\item Testing: Use Naive Bayes decision rule.  \hfill \\
		$ \displaystyle  h_{NB}(x) = \argmax_y P(y) \prod_{i=1}^{LengthDoc} P(x_i | y)$
\end{itemize}

\subsubsection{NB for continuous $X_i$}

\begin{itemize}
	\item $k$ is an index over all possible labels. 
	\item $i$ is the $i^{th}$ feature. Here it is the pixel.
	\item $j$ is the $j^{th}$ training example.  
	\item $X_i^j$ is the $i^{th}$ pixel in the $j^{th}$ training sample. 
	\item $Y^j$ is the label corresponding to the $j^{th}$ training example. 
	\item $y_k$ is the $k^{th}$ label
	\item $j$ is $j^{th}$ training example.  
	\item $\delta(x) = 1$ if x true, else 0. 
	\item $h$: the function that returns the best class.  % Erick 2/4/2016
\end{itemize}


Example: character recognition where the darkness of each pixel is continuous.  \hfill \\
\subsection{Gausian Naive Bayes (GNB) for continuous features}

Find parameter that makes all the data points most likely.
What parameters explain our data best?  \hfill \\
 \hfill \\


Naive Bayes continuous video:  \url{https://www.youtube.com/watch?v=r1in0YNetG8}

	$ \displaystyle P(X_i = x | Y = y_k) = \frac{1}{\sigma_{ik} \sqrt{2 \pi}} e^\frac{-(x- \mu_{ik})^2}{2 \sigma_{ik}^2}$  \hfill \\
\begin{itemize}
	\item $\mu_{ik}$ is the mean of the values for the $i^{th}$ feature for the $k^{th}$ class. \hfill \\
	\item $\sigma_{ik}$ is the standard deviation of the values for the $i^{th}$ feature for the $k^{th}$ class. 
\end{itemize}	
	
Sometimes we assume one or both of these:
\begin{itemize}
	\item variance is independent of $Y$ (i.e. $\sigma_i$)
	\item variance is independent of $X_i$ (i.e. $\sigma_k$)
\end{itemize}
If we assume both, we assume just one $\sigma$ without subscripts. \hfill \\   \hfill \\

Estimating parameters for discrete $Y$ and continuous $X_i$:  \hfill \\
\begin{itemize}
	\item \textbf{mean}:  $\displaystyle \widehat{\mu}_{ik} = \frac{1}{\sum_j \delta(Y^j = y_k)} \sum_j X_i^j \delta(Y^j = y_k)$
		\begin{itemize}
			\item first term: divide by the number of training examples that are of class k. 
			\item second term: summing the continuous input of pixel i for all examples 
				in the training set that match label k.  
			\item so this is just an average brightness for pixel $i$ given class $k$ using 
				all the training data.   
		\end{itemize}
	\item \textbf{variance}:  $\displaystyle  \widehat{\sigma}_{ik}^2 = \frac{1}{\sum_j \delta(Y^j = y_k) - 1} \sum_j (x_i^j - \widehat{\mu}_{ik})^2 \delta(Y^j = y_k)$
		\begin{itemize}
			\item first term: divide by the number of training points of class k minus 1. 
			\item second term: sum the squared difference in brightness of pixel $i$ 
				compared to the mean for that pixel and label. 
		\end{itemize}
\end{itemize}
We don't need to use a Gaussian for the prior, $P(y)$.  We aren't optimizing over it, so it is safe to count.  % audio week 5 

\subsection{When Bayes Classifier is Optimal:}
In Bayes we are learning the function $h$ that produces labels $Y$ based on inputs $\bm{X}$.  
More formally: \hfill \\
$h : \bm{X} \mapsto Y$, or \hfill \\
we are learning "the function h that maps features $\bm{X}$ to labels Y". \hfill \\ \hfill \\

If you know the true $\displaystyle P(Y | \bm{X})$, then \hfill \\
$\displaystyle h_{Bayes} = \argmax_y P(Y=y | \bm{X} = x)$.   \hfill \\  \hfill \\
Note the subscript is Bayes, not Naive Bayes; no assumption of conditional independence.
Also, the conditionality is back to likelihood instead of posterior.  \hfill \\  \hfill \\

Theorem: Bayes (not NB) classifier $h_{Bayes}$ is optimal.  \hfill \\
$error_{true}(h_{Bayes}) \leq error_{true}(h)$, $\forall h$ \hfill \\
\hfill \\
\textit{This is theoretical result: we don't know $P(\bm{x})$.  We can't calculate the true Bayes classifier b/c we don't know the distribution of all the data.)}
We also don't know $P(Y | \bm{X})$, the true class' highest probability.  Usually that's hidden; if we knew it we would go home happy.  


Plain english: the predictions you get from Bayes are better than any other function/prediction available. \hfill \\ \hfill \\
Proof:  
\begin{align*}  % changed notation to capital p, X, Y
	P_h(error) = \int_x P_h & (error | \bm{X}) P(\bm{X}) \\  
		& \mbox{           def. of error: } P_h(error| \bm{x}) = \int_y \delta(h(\bm{X}), Y) P(Y| \bm{X}) \\
		& \mbox{               (note, usually we'd sum over the classes, Y)}  \\
			= \int_x \int_y & \delta(h(\bm{X}), Y) P(Y | \bm{X}) P(\bm{X})  \\
			& \mbox{     (the double integral is zero when $P(Y | \bm{X})$ is largest,} \\
			& \mbox{     which is when the correct classification was selected.)}
\end{align*} 
We are averaging over novel data sets that are generated under the same conditions.

%Note that this is different notation than the rest.  Large vs small P.  
Different notation: delta has a comma in the parentheses and not an equality.   \hfill \\
\begin{itemize}
	\item $P_h(error)$ is probability of error across all classifications. 
	\item $\delta(h(\bm{X}, Y)$ is 1 if your X is classified right and 0 if not. 
	\item $P_h(error| \bm{x})$ is the probability that your classification is wrong.  
	\item $\int_x P_h (error | \bm{X}) P(\bm{X}) $ is the expectation of the errors.  
	\item $\delta(h(\bm{X}), Y)$ is 
\end{itemize}

Proof in words:  ??? 

Aside: note that for one classification y is not a vector.  It is a point. 
