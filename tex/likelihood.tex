\section{Maximum Likelihood \& Maximum a Posteriori}
\smallskip \hrule height 2pt \smallskip
(Also see paragraph at the end of this PDF near vocab for a MLE/MAP comparison.) \hfill \\

\underline{Vocab}
\begin{itemize}
	\item\textbf{likelihood}: the probability of the data given a parameter.  E.g. $P(D | \theta)$ (for discrete like Binomial).  
	Need not a pdf; need not be normalized.   %  http://www.robots.ox.ac.uk/~az/lectures/est/lect34.pdf  + Erick
 	\item \textbf{log-likelihood}: lower-case: $l(\theta|x) = \log L(\theta | x)$
	\item \textbf{maximum likelihood} (ML): 
	\item \textbf{MLE}: Maximum Likelihood Estimation. 
	\item \textbf{PAC}: Probability Approximately Correct. 
	\item \textbf{Posterior}: the likelihood times the prior, normalized  % Murphy 2012 pg 70
\end{itemize}
 
\underline{MLE}: Maximum Likelihood Estimation \hfill \\
Choose $\theta$ to maximize probability of D. \hfill \\
Set derivative of \_\_ to zero and solve.  If function is multivariate, set each partial derivative to zero and solve. \hfill \\
$\hat{\theta} = \argmax_\theta P(D | \theta) = \argmax_\theta \ln P(D | \theta) $ \hfill \\
Note we are using $\ln$, not $\log_2$ as we did for entropy above.  Want it to cancel exponents now. 
 \hfill \\
 
\hfill \\
\underline{Binomial Distribution} \hfill \\
Assumes i.i.d: $D=\{x_i | i=1 \dots n\}, P(D | \theta) = \prod_i P(x_i \mid \theta)$. \hfill \\
Likelihood function: $P(D | \theta) = \theta^{\alpha_H} (1-\theta)^{\alpha_T}$  \hfill \\
P(heads) = $\theta$, P(tails) = $1 - \theta$ \hfill \\
\begin{align*} 
	\hat{\theta} &= \argmax_\theta \ln P(D| \theta) \\
	 	 &= \argmax_\theta \ln \theta*{\alpha H}(1-\theta)^{\alpha T}
\end{align*}
Find optimal theta by setting the derivative to zero: 
\begin{align*} 
	\frac{d}{d\theta} \ln P(D| \theta) &= \frac{d}{d\theta}  \ln \theta*{\alpha H}(1-\theta)^{\alpha T} \\
	 	 &= \argmax_\theta \ln \theta*{\alpha H}(1-\theta)^{\alpha T} \\
		 & = \dots = \frac{\alpha_H}{\alpha_H  + \alpha_T}
\end{align*}

For Binomial, there is exponential decay in uncertainty with \# of observations.  % slide 7 at http://courses.cs.washington.edu/courses/cse446/16wi/Slides/3_PointEstimation.pdf
You can also find the probability that you are approximately correct (see \href{http://courses.cs.washington.edu/courses/cse446/16wi/Slides/3_PointEstimation.pdf}{notes}).  \hfill \\
$P(|\widehat{\theta} = \theta*| \geq \epsilon) \leq 2e^{-2N\epsilon^2}$.  Can calculate N (\# of flips) to have error less than $\epsilon$ with probability of being incorrect $\delta$.  Your sensitivity depends on your problem; error on stock market data might cost billions. 

What if you had prior beliefs?  Use MAP instead of MLE.

